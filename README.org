#+OPTIONS: ^:nil author:nil toc:nil
* yamlett - Yet Another Machine Learning Experiment Tracking Tool
:PROPERTIES:
:header-args:jupyter-python: :session yamlett :results value raw :kernel python3 :exports code :eval no-export
:END:

#+TOC: headlines 2 local

#+begin_export markdown
![PyPI](https://img.shields.io/pypi/v/yamlett)
![PyPI - Python Version](https://img.shields.io/pypi/pyversions/yamlett)
![PyPI - License](https://img.shields.io/pypi/l/yamlett)
#+end_export

** What is =yamlett=?
:PROPERTIES:
:CUSTOM_ID: what-is-yamlett
:END:

=yamlett= provides a simple but flexible way to track your ML experiments.

It has a simple interface with only two primitives: =Run= and =Experiment=.

- A =Run= is used to store information about one iteration of your =Experiment=.
  You can use it to record any ([[http://bsonspec.org][BSON]]-serializable) information you want such as
  model parameters, metrics, or pickled artifacts.
- An =Experiment= is a collection of =Run= objects. It has a =name= and it is a
  wrapper around a =pymongo.collection.Collection= object ([[https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection][reference]]), meaning
  that you can query it using =find= or =aggregate=. Think of it as a way to
  collect all the modeling iterations for a specific project.

The main difference with other tracking tools (e.g. MLflow) is that =yamlett=
lets you save complex structured information using dictionaries or lists and
filter on them later using MongoDB queries.

=yamlett= is particularly useful if your experiments are configuration-driven.
Once your configuration is loaded as a python object, storing it is as easy as
~run.store("config", config)~.

** Installation
:PROPERTIES:
:CUSTOM_ID: installation
:END:

=yamlett= can be installed with ~pip~:
#+begin_src sh :eval no
pip install yamlett
#+end_src

It also requires a MongoDB instance that you can connect to. If you don't have
one and just want to try out =yamlett=, we provide a [[file:docker-compose.yaml][docker compose file]] that
starts a MongoDB instance available at =localhost:27017= (along with instances
of [[https://prestodb.io][Presto]] and [[https://www.metabase.com][Metabase]]).

** Getting started

In =yamlett=, ~MongoClient~ [[https://pymongo.readthedocs.io/en/stable/api/pymongo/mongo_client.html#pymongo.mongo_client.MongoClient][connection parameters]] can be passed as keyword
arguments in both =Run= and =Experiment= to specify what MongoDB instance you
want to connect to. If you don't pass anything, the default arguments
(=localhost:27017=) will be used. If you have a custom MongoDB instance, you can
specify its ~host~ and ~port~ when creating a =Run= using src_python[:eval no
:export code]{run = Run(host="mymongo.host.com", port=27017)}.

** Example
:PROPERTIES:
:CUSTOM_ID: example
:END:

In this section, we compare the same model run but with two different tracking
different approaches: MLflow-like vs yamlett.

*** Set up the experiment
:PROPERTIES:
:CUSTOM_ID: set-up-experiment
:END:

First, let's load a dataset for a simple classification problem that ships with
scikit-learn.

#+begin_src jupyter-python
from sklearn.datasets import load_iris

X, y = load_iris(return_X_y=True)
#+end_src

#+RESULTS:

Then, we create a logistic regression model and train that model on the iris
dataset, increasing the number of iterations and changing the regularization
strength.

#+begin_src jupyter-python
from sklearn.linear_model import LogisticRegression

model = LogisticRegression(max_iter=200, C=0.1)
model.fit(X, y)
#+end_src

#+RESULTS:
: LogisticRegression(C=0.1, max_iter=200)

*** MLflow-like tracking
:PROPERTIES:
:CUSTOM_ID: mlflow-like-tracking
:END:

With =yamlett=, you are free to organize you tracking information so you could
decide to store it using a "flat" approach similar to MLflow where each key has
an associated value and there can be no nesting.

#+begin_src jupyter-python
from yamlett import Run
from sklearn.metrics import f1_score

run = Run(id="mlflow-like-run")

# store some information about your trained model: its class and its parameters
run.store("params_model_class", model.__class__.__name__)
for param_name, param_value in model.get_params().items():
    run.store(f"params_model_{param_name}", param_value)

# store information about your data
run.store("data_n_features", X.shape[0])
run.store("data_n_observations", X.shape[1])

# store the F1 score on the train data
run.store("metrics_train_f1_score", f1_score(y, model.predict(X), average="weighted"))

# you can even store a pickled version of your model on disk
run.store("model", model, pickled=True)
#+end_src

#+RESULTS:
# [goto error]

After running this code, we can retrieve the stored information by calling
~run.data(resolve=True)~:

#+begin_src jupyter-python :exports results :display plain :results scalar
from pprint import pprint

pprint(run.data(resolve=True))
#+end_src

#+RESULTS:
#+begin_example
{'_id': 'mlflow-like-run',
 'data_n_features': 150,
 'data_n_observations': 4,
 'metrics_train_f1_score': 0.9599839935974389,
 'model': LogisticRegression(C=0.1, max_iter=200),
 'params_model_C': 0.1,
 'params_model_class': 'LogisticRegression',
 'params_model_class_weight': None,
 'params_model_dual': False,
 'params_model_fit_intercept': True,
 'params_model_intercept_scaling': 1,
 'params_model_l1_ratio': None,
 'params_model_max_iter': 200,
 'params_model_multi_class': 'auto',
 'params_model_n_jobs': None,
 'params_model_penalty': 'l2',
 'params_model_random_state': None,
 'params_model_solver': 'lbfgs',
 'params_model_tol': 0.0001,
 'params_model_verbose': 0,
 'params_model_warm_start': False}
#+end_example

This approach is straightforward: one scalar for each key in the document.
However, one downside is that you need to maintain your own namespace
convention. For example here, we used underscores to separate the different
levels of information (params, data, metrics, etc) but this can quickly get
confusing if chosen incorrectly: is it =params/model/fit_intercept= or
=params/model_fit/intercept=? It is also more work than needed when information
already comes nicely organized (e.g. =model.get_params()=).

*** =yamlett= tracking
:PROPERTIES:
:CUSTOM_ID: yamlett-like-tracking
:END:

The method we propose in this package leverages Python dictionaries / NoSQL DB
documents to automatically store your information in a structured way. Let's see
what it looks like using the same run as above:

#+begin_src jupyter-python
from yamlett import Run
from sklearn.metrics import f1_score

run = Run(id="yamlett-run")

# store your model information
model_info = {
    "class": model.__class__.__name__,
    "params": model.get_params(),
}
run.store(f"model", model_info)

# store information about your data
run.store("data", {"n_features": X.shape[0], "n_observations": X.shape[1]})

# store the F1 score on your train data
run.store("metrics.f1_score", f1_score(y, model.predict(X), average="weighted"))

# you can even store a pickled version of your model on disk
run.store("model.artifact", model, pickled=True)
#+end_src

#+RESULTS:

Once again, let's call =run.data(resolve=True)= and see what information we stored:

#+begin_src jupyter-python :exports results :results scalar
from pprint import pprint

pprint(run.data(resolve=True))
#+end_src

#+RESULTS:
#+begin_example
{'_id': 'yamlett-run',
 'data': <Box: {'n_features': 150, 'n_observations': 4}>,
 'metrics': <Box: {'f1_score': 0.9599839935974389}>,
 'model': {'artifact': LogisticRegression(C=0.1, max_iter=200),
           'class': 'LogisticRegression',
           'params': {'C': 0.1,
                      'class_weight': None,
                      'dual': False,
                      'fit_intercept': True,
                      'intercept_scaling': 1,
                      'l1_ratio': None,
                      'max_iter': 200,
                      'multi_class': 'auto',
                      'n_jobs': None,
                      'penalty': 'l2',
                      'random_state': None,
                      'solver': 'lbfgs',
                      'tol': 0.0001,
                      'verbose': 0,
                      'warm_start': False}}}
#+end_example

The run information is now stored in a document that can be easily parsed based
on its structure. The top level keys of the document are =data=, =metrics=, and
=model= making it easier to find information than with long keys in a flat
dictionary. For instance, you may want to look at all the metrics for a given
run using ~run.data()["metrics"]~.

#+begin_src jupyter-python :exports results
pprint(run.data()["metrics"])
#+end_src

#+RESULTS:
: <Box: {'f1_score': 0.9599839935974389}>

Note that =yamlett= does not impose the document hierarchy so you are free to
organize your run data as you see fit. Additionally, because =yamlett= is a
light abstraction layer on top of MongoDB, you can query runs in an =Experiment=
using =find= or =aggregate=. For example, we can retrieve all runs in the
default experiment for which:
1. the model was fit with a bias term
2. on a dataset with at least 3000 data points
3. that yielded an F1 score of at least 0.9

#+begin_src jupyter-python
from yamlett import Experiment

e = Experiment()

e.find(
    {
        "model.params.fit_intercept": True,
        "data.n_observations": {"$gte": 3000},
        "metrics.f1_score": {"$gte": 0.9},
    }
)
#+end_src

#+RESULTS:
: <pymongo.cursor.Cursor at 0x7fb9935e3a50>


* Roadmap [11/17] :noexport:

- [X] Add basic unit tests
- [X] Add tests across python version using tox
  + tox replaced by Github Actions
- [X] Add CI
- [X] Add CD
- [X] Release 0.0.1 to github
- [X] Release to pypi
- [X] add description to pypi release
- [X] add installation guide
  + ~pip install~
  + needs mongodb instance
- [X] Add docstrings
- [X] Allow dotted notation for returned data
- [-] Enable artifacts to be stored on disk or in cloud storage
  + [X] Let users provide an object that supports =open=, =write=, and =read=
    and interacts with the file system
  + [X] provide ~data(resolve: bool)~ function
  + [X] store the data writer as a pickled object in mongodb
  + [X] update README (run.data -> run.data())
  + [ ] add tests
- [X] Add e2e runnable example
- [ ] add example storing a large artifact locally
- [ ] add example storing a large artifact using GCS
- [ ] Use environment variables to define MongoDB parameters
- [ ] publish documentation through rtd or github pages
- [ ] Add example for connecting to Metabase and Presto
  + metabase allows connecting to an instance of mongodb and query data
  + sql is more common so we can plug presto on top of mongodb and link metabase
    to presto
  + caveat that the schema cannot change when using Presto: ie no new fields in
    new runs
