* yamlett - Yet Another Machine Learning Experiment Tracking Tool
:PROPERTIES:
:header-args:jupyter-python: :session yamlett :results value raw :async yes
:END:

=yamlett= provides a simple but flexible way to track your ML experiments.

It has a simple interface with only two primitives: =Experiment= and =Run=.

- An =Experiment= is a named collection of =Run= objects. It is a wrapper around
  a =pymongo.collection.Collection= object ([[https://pymongo.readthedocs.io/en/stable/api/pymongo/collection.html#pymongo.collection.Collection][reference]]), meaning that you can
  query it using =find= or =aggregate=. Think of it as a way to collect all the
  modeling iterations of a specific project.
- A =Run= is used to store information about one iteration of your =Experiment=.
  You can use it to record any ([[http://bsonspec.org][BSON]]-serializable) information you want such as
  model parameters, metrics, or pickled artifacts.

The main difference with other tracking tools (e.g. MLflow) is that you can save
complex information using dictionaries or lists and filter on them later using
MongoDB queries. This is helpful to save the information about a run using
whatever organization you prefer. For instance, you could store your information
using a flat approach similar to MLflow (key/value pairs):
#+begin_src python :eval no
from yamlett import Run

run = Run()

# build your model
model = ...

# load your data
X, y = ...

with run:
    # store your parameters
    run.store("params_model_type", "Logistic Regression")
    run.store("params_model_pypath", "sklearn.linear_model.LogisticRegression")
    for param_name, param_value in model.get_params():
        run.store(f"params_model_{param_name}", param_value)

    # store information about your data
    run.store("data_n_features", X.shape[0])
    run.store("data_n_observations", X.shape[1])

# train your model
model.fit(X, y)

with run:
    # store your results and metrics
    run.store("metrics_f1_score", 0.92)

    # store your artifacts
    run.store("model", model, pickle=True)

#+end_src

After running this code, we can retrieve the stored information by calling
=run.data=:
#+begin_src python :eval no
{
    "params_model_type": "Logistic Regression",
    "params_model_pypath": "sklearn.linear_model.LogisticRegression",
    "params_model_C": 0.01,
    "params_model_fit_intercept": True,
    "data_n_features": 91,
    "data_n_observations": 3500,
    "metrics_f1_score": 0.92,
    "model": model
}
#+end_src

This approach is straightforward: one scalar for each key in the document.
However, one downside of this approach is that you need to maintain your own
namespace convention. For example here, we used underscores to separate the
different levels of information (params, data, metrics, etc) but this can
quickly get confusing: is it =params/model/fit_intercept= or
=params/model_fit/intercept= ? It's also a bit more work than expected when some
information comes in a dictionary as it requires unpacking it and doing some
string formatting (e.g. =model.get_params()=). Additionally, it can be hard to
find the value you are interested in once you have a large number of parameters
and metrics.

The method we propose in this package is to leverage Python dictionaries /
MongoDB documents to automatically store your information in a organized manner.
Let's see what it looks like using a similar run as the example above:

#+begin_src python :eval no
from yamlett import Run

run = Run()

# build your model
model = ...

# load your data
X, y = ...

with run:
    # store your parameters
    model_params = {
        "type": "Logistic Regression",
        "pypath": "sklearn.linear_model.LogisticRegression",
        **model.get_params(),
    }
    run.store(f"params.model", model_params)

    # store information about your data
    run.store("data", {"n_features": X.shape[0], "n_observations": X.shape[1]})

# train your model
model.fit(X, y)

with run:
    # store your results and metrics
    run.store("metrics.f1_score", 0.92)

    # store your artifacts
    run.store("model", model, pickle=True)
#+end_src

Once again, let's call =run.data= and see what information we stored:

#+begin_src python :eval no
{
    "params": {
        "model": {
            "type": "Logistic Regression",
            "pypath": "sklearn.linear_model.LogisticRegression",
            "C": 0.01,
            "fit_intercept": True,
        }
    },
    "data": {
        "n_features": 91,
        "n_observations": 3500,
    },
    "metrics": {
        "f1_score": 0.92,
    },
    "model": model,
}
#+end_src

The run information is now stored in a document that can be easily parsed based
on its organization. Additionally, because =yamlett= is built on top of MongoDB,
you can query runs in an =Experiment= using =find= or =aggregate=. For instance,
we could retrieve all runs in the default experiment for which:
1. the model was fit with bias term
2. on a dataset with at least 3000 data points
3. that yielded an F1 score of at least 0.9

#+begin_src python :eval no
from yamlett import Experiment

e = Experiment()

e.find(
    {
        "params.model.fit_intercept": True,
        "data.n_observations": {"$gte": 3000},
        "metrics.f1_score": {"$gte": 0.9},
    }
)
#+end_src

Note that =yamlett= does not enforced the document hierarchy so you are free to
organize your data as you see fit. Finally, =yamlett= is especially useful if
your experiments are configuration driven as you are then able to simply read
your configuration file and save it along your other results using
=run.store("config", config")=.

* Roadmap

** TODO Automatically load pickled objects
** TODO Add basic unit tests
** TODO Add tests across python version using tox
** TODO Add CI/CD
** TODO Release 0.1.0 to github
** TODO Release to pypi
** TODO Add e2e runnable example
** TODO Add example for connecting to Metabase and Presto
- metabase allows connecting to an instance of mongodb and query data
- sql is more common so we can plug presto on top of mongodb and link metabase
  to presto
- caveat that the schema cannot change when using Presto: ie no new fields in
  the runs

* Local Variables
# Local Variables:
# eval: (add-hook 'after-save-hook (lambda ()(org-babel-tangle)) nil t)
# End:

